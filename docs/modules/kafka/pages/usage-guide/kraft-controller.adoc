= KRaft mode (experimental)
:description: Apache Kafka KRaft mode with the Stackable Operator for Apache Kafka

WARNING: The Kafka KRaft mode is currently experimental, and subject to change.

Apache Kafka's KRaft mode replaces Apache ZooKeeper with Kafka’s own built-in consensus mechanism based on the Raft protocol.
This simplifies Kafka’s architecture, reducing operational complexity by consolidating cluster metadata management into Kafka itself.

WARNING: The Stackable Operator for Apache Kafka currently does not support automatic cluster upgrades from Apache ZooKeeper to KRaft.

== Overview

* Introduced: Kafka 2.8.0 (early preview, not production-ready).
* Matured: Kafka 3.3.x (production-ready, though ZooKeeper is still supported).
* Default & Recommended: Kafka 3.5+ strongly recommends KRaft for new clusters.
* Full Replacement: Kafka 4.0.0 (2025) removes ZooKeeper completely.
* Migration: Tools exist to migrate from ZooKeeper to KRaft, but new deployments should start with KRaft.

== Configuration

The Stackable Kafka operator introduces a new xref:concepts:roles-and-role-groups.adoc[role] in the KafkaCluster CRD called KRaft `Controller`.
Configuring the `Controller` will put Kafka into KRaft mode. Apache ZooKeeper will not be required anymore.

[source,yaml]
----
apiVersion: kafka.stackable.tech/v1alpha1
kind: KafkaCluster
metadata:
  name: kafka
spec:
  image:
    productVersion: "3.9.1"
  brokers:
    roleGroups:
      default:
        replicas: 1
  controllers:
    roleGroups:
      default:
        replicas: 3
----

NOTE: Using `spec.controllers` is mutually exclusive with `spec.clusterConfig.zookeeperConfigMapName`.

=== Recommendations

A minimal KRaft setup consisting of at least 3 Controllers has the following https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/[resource requirements]:

* `600m` CPU request
* `3000m` CPU limit
* `3000Mi` memory request and limit
* `6Gi` persistent storage

NOTE: The Controller replicas should sum up to an odd number for the Raft consensus.

=== Resources

Corresponding to the values above, the operator uses the following resource defaults:

[source,yaml]
----
controllers:
  config:
    resources:
      memory:
        limit: 1Gi
      cpu:
        min: 250m
        max: 1000m
      storage:
        logDirs:
          capacity: 2Gi
----

=== Overrides

The configuration of overrides, JVM arguments etc. is similar to the Broker and documented on the xref:concepts:overrides.adoc[concepts page].

== Internal operator details

KRaft mode requires major configuration changes compared to ZooKeeper:

* `cluster-id`: This is set to the `metadata.name` of the KafkaCluster resource during initial formatting
* `node.id`: This is a calculated integer, hashed from the `role` and `rolegroup` and added `replica` id.
* `process.roles`: Will always only be `broker` or `controller`. Mixed `broker,controller` servers are not supported.
* The operator configures a static voter list containing the controller pods. Controllers are not dynamically managed.

== Known Issues

* Automatic migration from Apache ZooKeeper to KRaft is not supported.
* Scaling controller replicas might lead to unstable clusters.
* Kerberos is currently not supported for KRaft in all versions.

== Troubleshooting

=== Cluster does not start

Check that at least a quorum (majority) of controllers are reachable.

=== Frequent leader elections

Likely caused by controller resource starvation or unstable Kubernetes scheduling.

=== Migration issues (ZooKeeper to KRaft)

Ensure Kafka version 3.9.x and higher and follow the official migration documentation.
The Stackable Kafka operator currently does not support the migration.

=== Scaling issues

The https://developers.redhat.com/articles/2024/11/27/dynamic-kafka-controller-quorum[Dynamic scaling] is only supported from Kafka version 3.9.0.
If you are using older versions, automatic scaling may not work properly (e.g. adding or removing controller replicas).

== Kraft migration guide

The operator version `26.3.0` adds support for migrating Kafka clusters from ZooKeeper to KRaft mode.

This guide describes the steps required to migrate an existing Kafka cluster managed by the Stackable Kafka operator from ZooKeeper to KRaft mode.

NOTE: Before starting the migration we recommend to reduce producer/consumer operations to a minimum or even pause them completely if possible to reduce the risk of data loss during the migration.

To make the migration step as clear as possible, we'll use a complete working example throughout this guide.
The example cluster will be kept minimal without any additional configuration.

We start by creating a dedicated namespace to work in and deploy the Kafka cluster including ZooKeeper and credentials.

[source,yaml]
----
include::example$kraft_migration/01-setup.yaml[]
----

=== Requirements

* Kafka clusters **must** be set up with the Stackable Kafka operator version `26.3.0` or higher. Kafka clusters set up with a previous operator version **cannot be upgraded** without migrating all broker instances first. This is because broker id management must be handed over from Kafka to the operator. The broker ids are not compatible between the two systems.
* Kafka version **must** be `3.7.2` or `3.9.1`. Starting with version `4.0.0` the Zookeeper support is removed completely.

=== 1. Start Kraft controllers

In this step we will perform the following actions:

1. Retrieve the current `cluster.id` as generated by Kafka.
2. Update the `KafkaCluster` resource to add `spec.controllers` property.
3. Configure the controllers to run in migration mode.
4. Apply the changes and wait for all cluster pods to become ready.

We can obtain the current `cluster.id` either by inspecting the ZooKeeper data or from `meta.properties` file on one of the brokers.
In this example, the identifier is `cPh4Fb3pRvyqiiVjaBDaEw`.
We add this value to the `KAFKA_CLUSTER_ID` environment variable for both brokers and controllers.

The complete example `KafkaCluster` resource after applying the required changes looks as follows:

[source,yaml]
----
include::example$kraft_migration/02-start-controllers.yaml[]
----

We `kubectl apply` the updated resource and wait for brokers and controllers to become ready.

=== 2. Migrate metadata

In this step we will perform the following actions:

1. Obtain the controller quorum configuration.
2. Enable metadata migration mode on the brokers.
3. Configure the controller quorum on the brokers.
4. Apply the changes and restart the broker pods.

The exact value of the quorum must be obtained from the `/tmp/controller.properties` file on one of the controller pods.
To start the metadata migration, we need to add the `zookeeper.metadata.migration.enable: "true"` and controller quorum configuration to the broker configuration.

For this step, the complete example `KafkaCluster` resource looks as follows:

[source,yaml]
----
include::example$kraft_migration/03-migrate-metadata.yaml[]
----

After we apply the changes, we then restart the brokers and wait for them to become ready again.

[source,bash]
----
kubectl rollout restart statefulset simple-kafka-broker-default -n kraft-migration
----

Finally we check that metadata migration was successful:

[source,bash]
----
kubectl logs -n kraft-migration simple-kafka-controller-default-2 | grep -i completed
...
[2025-12-22 09:23:53,372] INFO [KRaftMigrationDriver id=2110489705] Completed migration of metadata from ZooKeeper to KRaft. 0 records were generated in 102 ms across 0 batches. The average time spent waiting on a batch was -1.00 ms. The record types were {}. The current metadata offset is now 280 with an epoch of 3. Saw 0 brokers in the migrated metadata []. (org.apache.kafka.metadata.migration.KRaftMigrationDriver)
----

=== 3. Migrate brokers


NOTE: This is the last step before fully switching to KRaft mode. In case of unforeseen issues, it is the last step where we can roll back to ZooKeeper mode.

In this step we will perform the following actions:

1. Remove the migration properties from the previous step on the brokers.
2. Assign Kraft role properties to brokers.
3. Apply the changes and restart the broker pods.

We need to preserve the quorum configuration added in the previous step.

For this step, the complete example `KafkaCluster` resource looks as follows:


[source,yaml]
----
include::example$kraft_migration/04-migrate-brokers.yaml[]
----

=== 4. Enable Kraft mode

After this step, the cluster will be fully running in KRaft mode and it cannot be rolled back to ZooKeeper mode anymore.

In this step we will perform the following actions:

1. Put the cluster in Kraft mode by updating the `spec.clusterConfig.metadataManager` property.
2. Remove Kraft quorum configuration from the broker pods.
3. Remove the ZooKeeper migration flag from the controllers.
4. Apply the changes and restart all pods.

We need to preserve the `KAFKA_CLUSTER_ID` environment variable for the rest of the lifetime of this cluster.

The complete example `KafkaCluster` resource after applying the required changes looks as follows:

[source,yaml]
----
include::example$kraft_migration/05-kraft-mode.yaml[]
----

Verify that the cluster is healthy and consumer/producer operations work as expected.

=== 5. Cleanup

Before proceeding with this step please ensure that the Kafka cluster is fully operational in KRaft mode.

In this step we remove the now unused ZooKeeper cluster and related resources.

If the ZooKeeper cluster is also serving other use cases than Kafka you can skip this step.

In our example we can remove the ZooKeeper cluster and the Znode resource as follows:

[source,bash]
----
kubectl delete -n kraft-migration zookeeperznodes simple-kafka-znode
kubectl delete -n kraft-migration zookeeperclusters simple-zk
----

=== 6. Next steps

After successfully migrating to Kraft mode, consider updating the Kafka version to `4.0.0` or higher to benefit from the latest features and improvements in KRaft mode.
