= Stackable Operator for Apache Kafka
:description: Deploy and manage Apache Kafka clusters on Kubernetes using Stackable Operator.
:keywords: Stackable operator, Apache Kafka, Kubernetes, operator, SQL, engineer, broker, big data, CRD, StatefulSet, ConfigMap, Service, Druid, ZooKeeper, NiFi, S3, demo, version
:kafka: https://kafka.apache.org/
:github: https://github.com/stackabletech/kafka-operator/
:crd: {crd-docs-base-url}/kafka-operator/{crd-docs-version}/
:crd-kafkacluster: {crd-docs}/kafka.stackable.tech/kafkacluster/v1alpha1/
:feature-tracker: https://features.stackable.tech/unified
:metadata-quorum: https://cwiki.apache.org/confluence/display/KAFKA/KIP-500%3A+Replace+ZooKeeper+with+a+Self-Managed+Metadata+Quorum
:pegelonline: https://www.pegelonline.wsv.de/webservice/ueberblick
:earthquake: https://earthquake.usgs.gov/

[.link-bar]
* {github}[GitHub {external-link-icon}^]
* {feature-tracker}[Feature Tracker {external-link-icon}^]
* {crd}[CRD documentation {external-link-icon}^]

The Stackable operator for Apache Kafka is an operator that can deploy and manage {kafka}[Apache Kafka] clusters on Kubernetes.
Apache Kafka is a distributed streaming platform designed to handle large volumes of data in real-time.
It is commonly used for real-time data processing, data ingestion, event streaming, and messaging between applications.

== Getting started

Follow the xref:kafka:getting_started/index.adoc[] which will guide you through installing The Stackable Kafka and ZooKeeper operators, setting up ZooKeeper and Kafka and testing your Kafka using `kcat`.

== Resources

The _KafkaCluster_ custom resource contains your Kafka cluster configuration.
It defines a single `broker` xref:concepts:roles-and-role-groups.adoc[role].

image::kafka_overview.drawio.svg[A diagram depicting the Kubernetes resources created by the operator]

For every xref:concepts:roles-and-role-groups.adoc#_role_groups[role group] in the `broker` role the operator creates a StatefulSet.
Multiple Services are created - one at role level, one per role group as well as one for every individual Pod - to allow access to the entire Kafka cluster, parts of it or just individual brokers.

For every StatefulSet (role group) a ConfigMap is deployed containing a `log4j.properties` file for xref:usage-guide/logging.adoc[logging] configuration and a `server.properties` file containing the whole Kafka configuration which is derived from the KafkaCluster resource.

The operator creates a xref:concepts:service_discovery.adoc[] for the whole KafkaCluster which references the Service for the whole cluster.
Other operators use this ConfigMap to connect to a Kafka cluster simply by name and it can also be used by custom third party applications to find the connection endpoint.

== Dependencies

Kafka requires xref:zookeeper:index.adoc[Apache ZooKeeper] for coordination purposes (it will not be needed in the future as it will be replaced with a {metadata-quorum}[built-in solution]).

== Connections to other products

Since Kafka often takes on a bridging role, many other products connect to it.
In the <<demos, demos>> below you will find example data pipelines that use xref:nifi:index.adoc[Apache NiFi with the Stackable operator] to write to Kafka and xref:nifi:index.adoc[Apache Druid with the Stackable operator] to read from Kafka.
But you can also connect using xref:spark-k8s:index.adoc[Apache Spark] or with a custom Job written in various languages.

== [[demos]]Demos

xref:management:stackablectl:index.adoc[] supports installing xref:demos:index.adoc[] with a single command.
The demos are complete data piplines which showcase multiple components of the Stackable platform working together and which you can try out interactively.
Both demos below inject data into Kafka using NiFi and read from the Kafka topics using Druid.

=== Waterlevel demo

The xref:demos:nifi-kafka-druid-water-level-data.adoc[] demo uses data from {pegelonline}[PEGELONLINE] to visualize water levels in rivers and coastal regions of Germany from historic and real time data.

=== Earthquake demo

The xref:demos:nifi-kafka-druid-earthquake-data.adoc[] demo ingests {earthquake}[earthquake data] into a similar pipeline as is used in the waterlevel demo.

== Supported versions

The Stackable operator for Apache Kafka currently supports the Kafka versions listed below.
To use a specific Kafka version in your KafkaCluster, you have to specify an image - this is explained in the xref:concepts:product-image-selection.adoc[] documentation.
The operator also supports running images from a custom registry or running entirely customized images; both of these cases are explained under xref:concepts:product-image-selection.adoc[] as well.

include::partial$supported-versions.adoc[]

== Useful links

* The {github}[kafka-operator {external-link-icon}^] GitHub repository
* The operator feature overview in the {feature-tracker}[feature tracker {external-link-icon}^]
* The {crd-kafkacluster}[KafkaCluster {external-link-icon}^] CRD documentation
